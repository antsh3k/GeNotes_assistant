services:
  backend:
    build:
      context: .
      dockerfile: Dockerfile.backend
    container_name: genotes-backend
    restart: unless-stopped
    ports:
      - "8000:8000"
    volumes:
      - ./data:/data
      - ./backend:/app
    env_file:
      - .env
    environment:
      - OLLAMA_API_BASE_URL=http://ollama:11434
      - DATA_DIR=/data
      - CHROMA_DB_PATH=/data/chromadb
      - DATASETS_PATH=/data/datasets
      - PYTHONPATH=/app
      - PYTHONUNBUFFERED=1
      - DEBUG=1
    user: "${UID:-1000}:${GID:-1000}"
    working_dir: /app
    networks:
      - genotes-network
    depends_on:
      ollama:
        condition: service_started
      download-models:
        condition: service_started

  frontend:
    build:
      context: .
      dockerfile: Dockerfile.frontend
    container_name: genotes-frontend
    restart: unless-stopped
    ports:
      - "3000:80"
    volumes:
      - ./frontend:/app
      - /app/node_modules
    environment:
      - REACT_APP_API_URL=http://localhost:8000
    networks:
      - genotes-network
    depends_on:
      - backend

  nginx:
    image: nginx:1.25.3-alpine
    container_name: genotes-nginx
    restart: unless-stopped
    ports:
      - "80:80"
    volumes:
      - ./nginx/conf.d:/etc/nginx/conf.d
      - ./frontend/build:/usr/share/nginx/html
    depends_on:
      - frontend
      - backend
    networks:
      - genotes-network

  ollama:
    image: ollama/ollama:latest
    container_name: genotes-ollama
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    environment:
      - OLLAMA_HOST=0.0.0.0
    networks:
      - genotes-network

  download-models:
    image: curlimages/curl:latest
    container_name: genotes-download-models
    command: >
      /bin/sh -c '
      echo "Waiting for Ollama to be ready...";
      until curl -s http://ollama:11434/api/tags > /dev/null; do
        echo "Ollama is not ready yet. Waiting...";
        sleep 5;
      done;
      echo "Downloading models...";
      for model in nomic-embed-text mxbai-embed-large llama3.2:3b; do
        echo "Pulling $$model...";
        curl -X POST http://ollama:11434/api/pull -d "{\"name\": \"$$model\"}";
        echo -e "\\nFinished pulling $$model\\n";
      done;
      echo "All models downloaded successfully!"'
    networks:
      - genotes-network
    depends_on:
      ollama:
        condition: service_started
volumes:
  ollama_data:
    name: genotes-ollama-data

networks:
  genotes-network:
    driver: bridge
